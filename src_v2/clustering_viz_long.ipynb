{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - AE model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import umap\n",
    "import numpy as np\n",
    "from os import path\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold.t_sne import TSNE\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directories\n",
    "homedir = os.path.expanduser('~')\n",
    "prjdir = 'data1/stratification_ILRM'\n",
    "datadir = 'data'\n",
    "outdir = path.join(homedir, prjdir, datadir, 'mixed/cohorts/2019-1-8-12-25-16')\n",
    "expdir = path.join('/home/riccardo/data1/projects/ehr-stratification/data/experiments/mixed-noact-relu-15')\n",
    "\n",
    "# sub-sampling\n",
    "n_samples = None\n",
    "\n",
    "# read encoded vectors file and ordered medical record numbers\n",
    "with open(path.join(expdir, 'mrns.csv')) as f:\n",
    "    rd = csv.reader(f)\n",
    "    mrns = [r[0] for r in rd]\n",
    "\n",
    "with open(path.join(expdir, 'encoded_vect.csv')) as f:\n",
    "    rd = csv.reader(f)\n",
    "    encoded = [list(map(float, r)) for r in rd]\n",
    "\n",
    "# sub-sample the collection\n",
    "if n_samples is not None:\n",
    "    idx = [i for i in range(len(mrns))]\n",
    "    random.shuffle(idx)\n",
    "    idx = idx[:n_samples]\n",
    "    mrn_tmp = [mrns[i] for i in idx]\n",
    "    enc_tmp = [encoded[i] for i in idx]\n",
    "    mrns = mrn_tmp\n",
    "    encoded = enc_tmp\n",
    "set_mrns = set(mrns)\n",
    "    \n",
    "# read the vocabulary\n",
    "with open(path.join(outdir, 'cohort-new_vocab.csv')) as f:    \n",
    "    rd = csv.reader(f)\n",
    "    next(rd)\n",
    "    vocab = {r[1]: r[0] for r in rd}\n",
    "len_vocab = len(vocab)\n",
    "\n",
    "# read raw data\n",
    "with open(path.join(outdir, 'cohort-new_ehr.csv')) as f:\n",
    "    rd = csv.reader(f)\n",
    "    raw_ehr = {}\n",
    "    for r in rd:\n",
    "        if r[0] in set_mrns:\n",
    "            raw_ehr.setdefault(r[0], list()).extend(list(map(int, r[1::])))\n",
    "\n",
    "# raw data (scaled) counts\n",
    "scaler = StandardScaler()\n",
    "data = raw_ehr.values()\n",
    "mrn_list = list(raw_ehr.keys())\n",
    "raw_data = np.zeros((len(data), len_vocab))\n",
    "for idx, token_list in enumerate(data):\n",
    "    for t in token_list:\n",
    "        raw_data[idx, t - 1] += 1\n",
    "\n",
    "raw_data_scaled = scaler.fit_transform(raw_data)\n",
    "\n",
    "# get the list of diagnosed diseases associated with mrns\n",
    "with open(path.join(outdir, 'cohort-mrn_diseases.csv')) as f:\n",
    "    rd = csv.reader(f)\n",
    "    mrn_disease = {r[0]: r[1::] for r in rd if r[0] in set_mrns}\n",
    "\n",
    "# evaluate potential disease classes\n",
    "\n",
    "# (1) first diagnosis\n",
    "gt_disease = {m: mrn_disease[m][0] for m in mrn_disease}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">`svd_tfidf():` returns the SVD matrix of the TFIDF matrix of the raw ehr data;\n",
    "\n",
    ">`silhouette_analysis():` hierarchical clustering on input data maximizing the Silhouette Index\n",
    "\n",
    ">`single_plot():` one plot of all the clusters\n",
    "\n",
    ">`nonoveralp_plot():` N different plots, one per cluster, with N = no. of clusters\n",
    "\n",
    "> `outer_clustering_analysis():` external validation of clustering (entropy and Purity scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering parameters\n",
    "# Frequency count analysis parameters\n",
    "ehr_strat_model = True\n",
    "\n",
    "if ehr_strat_model == True:\n",
    "    HCpar = {'linkage_clu':'complete',\n",
    "             'affinity_clu':'cosine',\n",
    "             'min_cl':2,\n",
    "             'max_cl':11}\n",
    "else:\n",
    "    HCpar = {'linkage_clu':'complete',\n",
    "             'affinity_clu':'euclidean',\n",
    "             'min_cl':2,\n",
    "             'max_cl':11}\n",
    "FRpar = {'n_terms':10,\n",
    "        'ty_terms':['icd9', 'medication']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze clustering using silhouette scores\n",
    "def silhouette_analysis(data,\n",
    "                        min_clu=HCpar['min_cl'],\n",
    "                        max_clu=HCpar['max_cl']):\n",
    "    # bound analysis range\n",
    "    if min_clu < 2:\n",
    "        min_clu = 2\n",
    "\n",
    "    # run analysis for every clustering size\n",
    "    best_silh = 0\n",
    "    silh_scores = []\n",
    "    for n in range(min_clu, max_clu, 1):\n",
    "        hclu = AgglomerativeClustering(n_clusters=n,\n",
    "                                       linkage=HCpar['linkage_clu'],\n",
    "                                       affinity=HCpar['affinity_clu'])\n",
    "        lbl = hclu.fit_predict(data).tolist()\n",
    "        silh = silhouette_score(data, lbl, metric=HCpar['affinity_clu'])\n",
    "        if silh < 0:\n",
    "            break\n",
    "        print(' -- {0}: {1:.3f}'.format(n, silh))\n",
    "        silh_scores.append(silh)\n",
    "        if silh > best_silh:\n",
    "            best_silh = silh\n",
    "            n_clu = n\n",
    "            label = lbl\n",
    "    try:\n",
    "        print('No. of clusters: {0} -- Silhouette Score: {1:.3f}\\n'.format(\n",
    "            n_clu, best_silh))\n",
    "\n",
    "    except UnboundLocalError:\n",
    "        hclu = AgglomerativeClustering(n_clusters=min_clu,\n",
    "                                       linkage=HCpar['linkage_clu'],\n",
    "                                       affinity=HCpar['affinity_clu'])\n",
    "        n_clu = min_clu\n",
    "        label = hclu.fit_predict(data).tolist()\n",
    "        print('No. of Clusters: {0} -- Silhouette Score: {1:.3f}\\n'.format(\n",
    "            n_clu, best_silh))\n",
    "\n",
    "    plt.plot(range(min_clu, max_clu, 1), silh_scores)\n",
    "    return (n_clu, label, silh_scores)\n",
    "\n",
    "\n",
    "# SVD matrix of the TFIDF matrix of the raw ehr data\n",
    "def svd_tfidf(data, len_vocab, n_dimensions=200):\n",
    "    # apply tf-idf\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf_mtx = tfidf.fit_transform(data)\n",
    "\n",
    "    # reduce size of the matrix\n",
    "    svd = TruncatedSVD(n_components=n_dimensions)\n",
    "    svd_mtx = svd.fit_transform(tfidf_mtx)\n",
    "\n",
    "    return svd_mtx\n",
    "\n",
    "\n",
    "# one plot with all the clusters\n",
    "def single_plot(data, mrn_disease, colors):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for cl in set(mrn_disease):\n",
    "        x = [d[0] for j, d in enumerate(data) if mrn_disease[j] == cl]\n",
    "        y = [d[1] for j, d in enumerate(data) if mrn_disease[j] == cl]\n",
    "        cols = [c for j, c in enumerate(colors) if mrn_disease[j] == cl]\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.scatter(x,y,c=cols, label=cl)\n",
    "    plt.legend()\n",
    " \n",
    "    \n",
    "# non-overlapping plots, one per cluster\n",
    "def nonoverlap_plot(data, mrn_disease, colors):\n",
    "    fig, ax = plt.subplots(len(set(mrn_disease)), 1, figsize=(20, 10*len(set(mrn_disease))))\n",
    "    for idx, cl in enumerate(set(mrn_disease)):\n",
    "        x = [d[0] for j, d in enumerate(data) if mrn_disease[j] == cl]\n",
    "        y = [d[1] for j, d in enumerate(data) if mrn_disease[j] == cl]\n",
    "        cols = [c for j, c in enumerate(colors) if mrn_disease[j] == cl]\n",
    "        ax[idx].set_xticks([])\n",
    "        ax[idx].set_yticks([])\n",
    "        ax[idx].scatter(x, y, c=cols, label=cl)\n",
    "        ax[idx].legend()\n",
    "\n",
    "        \n",
    "# external clustering analysis\n",
    "def outer_clustering_analysis(data, gt_clu):\n",
    "    label_clu = sorted(set(gt_clu))\n",
    "\n",
    "    # format clustering ground truth\n",
    "    didx = {d: i for i, d in enumerate(label_clu)}\n",
    "    idxd = {i:d for d, i in didx.items()}\n",
    "    gt = [didx[d] for d in gt_clu]\n",
    "\n",
    "    # validate cluster number\n",
    "    if len(label_clu) == 1:\n",
    "        n_clu = 3\n",
    "    else:\n",
    "        n_clu = len(label_clu)\n",
    "\n",
    "    # run clustering\n",
    "    hclust = AgglomerativeClustering(n_clusters=n_clu,\n",
    "                                     linkage=HCpar['linkage_clu'],\n",
    "                                     affinity=HCpar['affinity_clu'])\n",
    "    clusters = hclust.fit_predict(data).tolist()\n",
    "\n",
    "    # count cluster occurrences\n",
    "    cnt_clu = [0] * n_clu\n",
    "    for c in clusters:\n",
    "        cnt_clu[c] += 1\n",
    "    class_clu = [[0] * n_clu for _ in range(len(label_clu))]\n",
    "    for i, gi in enumerate(gt):\n",
    "        class_clu[gi][clusters[i]] += 1\n",
    "\n",
    "    # compute entropy and purity\n",
    "    entropy = 0\n",
    "    purity = 0\n",
    "    for j in range(0, max(clusters) + 1):\n",
    "        en = 0\n",
    "        pu = []\n",
    "        for i in range(0, max(gt) + 1):\n",
    "            pij = class_clu[i][j] / cnt_clu[j]\n",
    "            pu.append(pij)\n",
    "            if pij != 0:\n",
    "                en += -(pij * np.log2(pij))\n",
    "        max_pu = max(pu)\n",
    "        ds_max = []\n",
    "        for idx, p in enumerate(pu):\n",
    "            if p == max_pu:\n",
    "                ds_max.append(idxd[idx])\n",
    "        print(\n",
    "            'Cluster: {0} -- '\n",
    "            'Entropy: {1:.3f}, '\n",
    "            'Purity: {2:.3f}'.format(j, en, max_pu))\n",
    "        for d in ds_max:\n",
    "            print(\"max(P) in cluster disease {0}\".format(d))\n",
    "        cweight = cnt_clu[j] / len(gt)\n",
    "        entropy += cweight * en\n",
    "        purity += cweight * max_pu\n",
    "\n",
    "    print('Average Entropy: {0:.2f}'.format(entropy))\n",
    "    print('Average Purity: {0:.2f}'.format(purity))\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "#Input: ehr lists corresponding to a cluster \n",
    "#Output: dictionary of term counts\n",
    "def FreqDict(tokens):\n",
    "    freq_dict = {}\n",
    "    for seq in tokens:\n",
    "        for s in seq:\n",
    "            if s not in freq_dict:\n",
    "                freq_dict[s] = 1\n",
    "            else:\n",
    "                freq_dict[s] += 1\n",
    "    return freq_dict\n",
    "#Input: dictionary cluster:ehrs; list mrns\n",
    "#Output:\n",
    "def freq_term(data, pred_class):\n",
    "    list_terms = []\n",
    "    for subc in range(len(set(pred_class))):\n",
    "        tmp_data = {}\n",
    "        for j in range(len(pred_class)):\n",
    "            if pred_class[j] == subc:\n",
    "                tmp_data.setdefault(subc, list()).append([rd for rd in data[j] \n",
    "                                                           if rd!=0 and \n",
    "                                                           (str.split(vocab[str(rd)], \"::\")[0] \n",
    "                                                           in FRpar['ty_terms'])])\n",
    "        print(\"Cluster {0} numerosity: {1}\".format(subc, len(tmp_data[subc])))\n",
    "        term_count = FreqDict(tmp_data[subc])\n",
    "        clust_mostfreq = []\n",
    "        for l in range(FRpar['n_terms']):\n",
    "            try:\n",
    "                MFMT = max(term_count, key=(lambda key: term_count[key]))\n",
    "                num_MFMT = 0\n",
    "                n_subj = 0\n",
    "                for ehr in tmp_data[subc]:\n",
    "                    if MFMT in ehr:\n",
    "                        n_subj += 1\n",
    "                for _, seq in raw_ehr.items():\n",
    "                    for t in seq:\n",
    "                        if t == MFMT:\n",
    "                            num_MFMT += 1\n",
    "                print(\"% most frequent term:{1} \"\n",
    "                       \"= {2:.2f} ({3} out of {4} terms in the whole dataset\"\n",
    "                       \"-- N patients in cluster {5})\".format(subc,\n",
    "                                                             vocab[str(MFMT)], \n",
    "                                                             term_count[MFMT]/num_MFMT, \n",
    "                                                             term_count[MFMT],\n",
    "                                                             num_MFMT,\n",
    "                                                             n_subj))\n",
    "                term_count.pop(MFMT)\n",
    "                clust_mostfreq.append(MFMT)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        print(\"\\n\")\n",
    "        list_terms.append(clust_mostfreq)\n",
    "    return list_terms\n",
    "\n",
    "##Hierarchical clustering function. Max silhouette.\n",
    "def hclust_ehr(data):\n",
    "    best_silh = -1\n",
    "    list_silh = []\n",
    "    for nc in range(HCpar['min_cl'],HCpar['max_cl'],1):\n",
    "        hclust = AgglomerativeClustering(n_clusters=nc, \n",
    "                                         linkage=HCpar['linkage_clu'], \n",
    "                                         affinity=HCpar['affinity_clu'])\n",
    "        tmp_label = hclust.fit_predict(data).tolist()\n",
    "        tmp_silh = silhouette_score(data, tmp_label, metric=HCpar['affinity_clu'])\n",
    "        print(nc, tmp_silh)\n",
    "        list_silh.append(float(tmp_silh))\n",
    "        if tmp_silh > best_silh:\n",
    "            best_silh = tmp_silh\n",
    "            n_clust = nc\n",
    "            label = tmp_label\n",
    "    try:\n",
    "        print(\"Number of clusters found:{0}, Silhouette score:{1:.3f}\\n\".format(n_clust, best_silh))\n",
    "    except UnboundLocalError:\n",
    "        hclust = AgglomerativeClustering(n_clusters=HCpar['min_cl'],\n",
    "                                         linkage=HCpar['linkage_clu'],\n",
    "                                         affinity=HCpar['affinity_clu'])\n",
    "        n_clust = HCpar['min_cl']\n",
    "        label = hclust.fit_predict(data).tolist()\n",
    "        best_silh = silhouette_score(data, label)\n",
    "        print(\"Number of clusters found:{0}, Silhouette score:{1:.3f}\\n\".format(n_clust, best_silh))\n",
    "    return n_clust, label, list_silh\n",
    "\n",
    "def chi_test(data, new_classes, term, mrns):\n",
    "    count_mat = np.zeros((2, len(set(new_classes))))\n",
    "    for c in set(new_classes):\n",
    "        for idx, m in enumerate(mrns):\n",
    "            if new_classes[idx] == c:\n",
    "                if term in data[idx]:\n",
    "                    count_mat[1][c] += 1\n",
    "                else:\n",
    "                    count_mat[0][c] += 1\n",
    "    print(\"Count matrix:\\n {0}\".format(count_mat))\n",
    "    chi2_stat, p_val, dof, ex = stats.chi2_contingency(count_mat)\n",
    "    string = \"Chi-squared test statistics: chi2_stat = {0} -- p_val = {1} -- dof = {2}\".format(\n",
    "                                                                  chi2_stat,\n",
    "                                                                  p_val,\n",
    "                                                                  dof)#row = classes, columns = vocab\n",
    "    print(string)\n",
    "    \n",
    "##Internal clustering validation\n",
    "def inner_clustering_analysis(disease_class, data, mrns, viz_data):\n",
    "    dis_viz_data = []\n",
    "    subclass_dis = []\n",
    "    for dis in sorted(set(disease_class)):\n",
    "        tmp_data = []\n",
    "        tmp_mrn = []\n",
    "        tmp_raw_ehr = []\n",
    "        for idx, d in enumerate(disease_class):\n",
    "            if d == dis:\n",
    "                dis_viz_data.append(viz_data[idx])\n",
    "                tmp_data.append(data[idx])\n",
    "                tmp_mrn.append(mrns[idx])\n",
    "                tmp_raw_ehr.append(raw_ehr[mrns[idx]])\n",
    "        print(\"Inspecting disease: {0}\\n\".format(dis))\n",
    "        n_clust, label, _ = hclust_ehr(tmp_data)\n",
    "        subclass_dis.extend([dis + ': subclust ' + str(l) for l in label])\n",
    "        list_terms = freq_term(tmp_raw_ehr, label)\n",
    "        for l in range(len(set(label))):\n",
    "            for lt in range(len(list_terms[l])):\n",
    "                print(\"Odds ratio chi2 test for cluster {0}\"\n",
    "                      \"term: {1}\".format(l, vocab[str(list_terms[l][lt])]))\n",
    "                try:\n",
    "                    chi_test(tmp_raw_ehr, label, list_terms[l][lt], tmp_mrn)\n",
    "                except ValueError:\n",
    "                    print(\"empty class(es)\")\n",
    "                    pass\n",
    "            print(\"\\n\\n\")\n",
    "    return(dis_viz_data, subclass_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run t-SNE for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize T-SNE\n",
    "# reducer = umap.UMAP(n_neighbors=20, min_dist=0.001, metric = 'manhattan')\n",
    "tsne = TSNE(n_components=2, n_iter=5000, perplexity=40, random_state=42, \n",
    "            init='pca')\n",
    "\n",
    "# plot colors\n",
    "col_dict = matplotlib.colors.CSS4_COLORS\n",
    "c_out = ['mintcream', 'cornsilk', 'lavenderblush', 'aliceblue', 'antiquewhite', 'aqua', 'aquamarine', 'azure', 'beige', 'powderblue', 'floralwhite', 'ghostwhite',\n",
    " 'lightcoral', 'lightcyan', 'lightgoldenrodyellow', 'lightgray', 'lightgreen', 'lightgrey', 'lightpink', 'lightsalmon', 'lightseagreen', 'lightskyblue',\n",
    " 'lightslategray', 'lightslategrey', 'lightsteelblue', 'lightyellow', 'linen', 'palegoldenrod', 'palegreen', 'paleturquoise', 'palevioletred', 'papayawhip',\n",
    " 'peachpuff', 'mistyrose', 'lemonchiffon', 'lightblue', 'seashell', 'white', 'blanchedalmond', 'oldlace', 'moccasin', 'snow', 'darkgray',\n",
    " 'ivory']\n",
    "colormap = [c for c in col_dict if c not in c_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE on the CNN encoded vectors\n",
    "# encoded = preprocessing.scale(encoded)\n",
    "# encoded_umap = reducer.fit_transform(encoded).tolist()\n",
    "encoded_tsne = tsne.fit_transform(encoded).tolist()\n",
    "\n",
    "print('Computed: CNN - AE encoded vectors tSNE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE on the raw vectores\n",
    "# raw_tsne = tsne.fit_transform(raw_data_scaled).tolist()\n",
    "# print(\"Computed: raw count data t-SNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE on the TF-IDF + SVD matrix\n",
    "svd_mat = svd_tfidf(raw_data, len_vocab, n_dimensions = 100)\n",
    "tfidf_tsne = tsne.fit_transform(svd_mat).tolist()\n",
    "print(\"Computed: TF-IDF matrix tSNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE on the LSTM encoded vectors\n",
    "# lstm_encoded_tsne = tsne.fit_transform(lstm_encoded_vect).tolist()\n",
    "# print(\"Computed LSTM encoded vectors t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE visualization and external clustering validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the disease classes: first_disease, oth_disease\n",
    "disease_class_first = [gt_disease[m] for m in mrns]\n",
    "raw_disease_class_first = [gt_disease[m] for m in mrn_list]\n",
    "disease_dict = {d: i for i, d in enumerate(set(disease_class_first))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN - AE encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "colors_en1 = [colormap[disease_dict[v]] for v in disease_class_first]\n",
    "single_plot(encoded_tsne, disease_class_first, colors_en1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cluster results\n",
    "clusters = outer_clustering_analysis(encoded, disease_class_first)\n",
    "colors_en2 = [colormap[v] for v in clusters]\n",
    "single_plot(encoded_tsne, clusters, colors_en2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _ = silhouette_analysis(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner clustering analysis\n",
    "encoded_subplots, en_sub_clust = inner_clustering_analysis(disease_class_first, encoded, mrns, encoded_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_new_disease_dict = {}\n",
    "for idx, nd in enumerate(set(en_sub_clust)):\n",
    "    encoded_new_disease_dict[nd] = idx\n",
    "colors_en3 = [colormap[encoded_new_disease_dict[v]] for v in en_sub_clust]\n",
    "single_plot(encoded_subplots, en_sub_clust, colors_en3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "# colors_raw1 = [colormap[disease_dict[v]] for v in disease_class_first]\n",
    "# single_plot(raw_tsne, disease_class_first, colors_raw1)\n",
    "\n",
    "# plot cluster results\n",
    "# clusters = outer_clustering_analysis(raw_data_scaled, gt_disease.values())\n",
    "# colors_raw2 = [colormap[v] for v in clusters]\n",
    "# single_plot(raw_tsne, clusters, colors_raw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "colors_tfidf1 = [colormap[disease_dict[v]] for v in raw_disease_class_first]\n",
    "single_plot(tfidf_umap, raw_disease_class_first, colors_tfidf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cluster results\n",
    "clusters = outer_clustering_analysis(svd_mat, raw_disease_class_first)\n",
    "colors_tfidf2 = [colormap[v] for v in clusters]\n",
    "single_plot(tfidf_umap, clusters, colors_tfidf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_analysis(tfidf_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_subplots, tfidf_sub_clust = inner_clustering_analysis(raw_disease_class_first, svd_mat, mrn_list, tfidf_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_new_disease_dict = {}\n",
    "for idx, nd in enumerate(set(tfidf_sub_clust)):\n",
    "    tfidf_new_disease_dict[nd] = idx\n",
    "colors_count3 = [colormap[tfidf_new_disease_dict[v]] for v in tfidf_sub_clust]\n",
    "single_plot(tfidf_subplots, tfidf_sub_clust, colors_count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
