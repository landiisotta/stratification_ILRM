{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import umap\n",
    "import numpy as np\n",
    "from os import path\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold.t_sne import TSNE\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##global variables\n",
    "FRpar = {'n_terms':20,\n",
    "         'ty_terms':['icd9', 'medication', 'lab', 'cpt', 'procedure']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze clustering using silhouette scores\n",
    "def silhouette_analysis(data,\n",
    "                        min_clu,\n",
    "                        max_clu,\n",
    "                        linkage,\n",
    "                        affinity,\n",
    "                        preproc=False):\n",
    "    if preproc:\n",
    "        data = preprocessing.scale(data)\n",
    "    # bound analysis range\n",
    "    if min_clu < 2:\n",
    "        min_clu = 2\n",
    "\n",
    "    # run analysis for every clustering size\n",
    "    best_silh = 0\n",
    "    silh_scores = []\n",
    "    for n in range(min_clu, max_clu, 1):\n",
    "        hclu = AgglomerativeClustering(n_clusters=n,\n",
    "                                       linkage=linkage,\n",
    "                                       affinity=affinity)\n",
    "        lbl = hclu.fit_predict(data).tolist()\n",
    "        silh = silhouette_score(data, lbl, metric=affinity)\n",
    "        if silh < 0:\n",
    "            break\n",
    "        print(' -- {0}: {1:.3f}'.format(n, silh))\n",
    "        silh_scores.append(silh)\n",
    "        if silh > best_silh:\n",
    "            best_silh = silh\n",
    "            n_clu = n\n",
    "            label = lbl\n",
    "    try:\n",
    "        print('No. of clusters: {0} -- Silhouette Score: {1:.3f}\\n'.format(\n",
    "            n_clu, best_silh))\n",
    "\n",
    "    except UnboundLocalError:\n",
    "        hclu = AgglomerativeClustering(n_clusters=min_clu,\n",
    "                                       linkage=linkage,\n",
    "                                       affinity=affinity)\n",
    "        n_clu = min_clu\n",
    "        label = hclu.fit_predict(data).tolist()\n",
    "        print('No. of Clusters: {0} -- Silhouette Score: {1:.3f}\\n'.format(\n",
    "            n_clu, best_silh))\n",
    "\n",
    "    return (n_clu, label, silh_scores)\n",
    "\n",
    "\n",
    "# SVD matrix of the TFIDF matrix of the raw ehr data\n",
    "def svd_count(data, len_vocab, n_dimensions=200, tfidf=False):\n",
    "    if tfidf:\n",
    "    # apply tf-idf\n",
    "        tfidf = TfidfTransformer()\n",
    "        mtx = tfidf.fit_transform(data)\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "        mtx = scaler.fit_transform(data)\n",
    "    # reduce size of the matrix\n",
    "    svd = TruncatedSVD(n_components=n_dimensions)\n",
    "    svd_mtx = svd.fit_transform(mtx)\n",
    "\n",
    "    return svd_mtx\n",
    "\n",
    "\n",
    "# one plot with all the clusters\n",
    "def single_plot(data, mrn_disease, colors, name):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for cl in set(mrn_disease):\n",
    "        x = [d[0] for j, d in enumerate(data) if mrn_disease[j] == cl]\n",
    "        y = [d[1] for j, d in enumerate(data) if mrn_disease[j] == cl]\n",
    "        cols = [c for j, c in enumerate(colors) if mrn_disease[j] == cl]\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.scatter(x,y,c=cols, label=cl)\n",
    "    plt.legend(loc=1)\n",
    "    plt.savefig(name) \n",
    "    \n",
    "# non-overlapping plots, one per cluster\n",
    "def nonoverlap_plot(data, mrn_disease, colors):\n",
    "    fig, ax = plt.subplots(len(set(mrn_disease)), 1, figsize=(20, 10*len(set(mrn_disease))))\n",
    "    for idx, cl in enumerate(set(mrn_disease)):\n",
    "        x = [d[0] for j, d in enumerate(data) if mrn_disease[j] == cl]\n",
    "        y = [d[1] for j, d in enumerate(data) if mrn_disease[j] == cl]\n",
    "        cols = [c for j, c in enumerate(colors) if mrn_disease[j] == cl]\n",
    "        ax[idx].set_xticks([])\n",
    "        ax[idx].set_yticks([])\n",
    "        ax[idx].scatter(x, y, c=cols, label=cl)\n",
    "        ax[idx].legend()\n",
    "        \n",
    "# external clustering analysis\n",
    "def outer_clustering_analysis(data, gt_clu, \n",
    "                              linkage, \n",
    "                              affinity, \n",
    "                              preproc=False):\n",
    "    \n",
    "    if preproc:\n",
    "        data = preprocessing.scale(data)\n",
    "        \n",
    "    label_clu = sorted(set(gt_clu))\n",
    "\n",
    "    # format clustering ground truth\n",
    "    didx = {d: i for i, d in enumerate(label_clu)}\n",
    "    idxd = {i:d for d, i in didx.items()}\n",
    "    gt = [didx[d] for d in gt_clu]\n",
    "\n",
    "    # validate cluster number\n",
    "    if len(label_clu) == 1:\n",
    "        n_clu = 3\n",
    "    else:\n",
    "        n_clu = len(label_clu)\n",
    "\n",
    "    # run clustering\n",
    "    hclust = AgglomerativeClustering(n_clusters=n_clu,\n",
    "                                     linkage=linkage,\n",
    "                                     affinity=affinity)\n",
    "    clusters = hclust.fit_predict(data).tolist()\n",
    "\n",
    "    # count cluster occurrences\n",
    "    cnt_clu = [0] * n_clu\n",
    "    for c in clusters:\n",
    "        cnt_clu[c] += 1\n",
    "    class_clu = [[0] * n_clu for _ in range(len(label_clu))]\n",
    "    for i, gi in enumerate(gt):\n",
    "        class_clu[gi][clusters[i]] += 1\n",
    "\n",
    "    # compute entropy and purity\n",
    "    entropy = 0\n",
    "    purity = 0\n",
    "    for j in range(0, max(clusters) + 1):\n",
    "        en = 0\n",
    "        pu = []\n",
    "        for i in range(0, max(gt) + 1):\n",
    "            pij = class_clu[i][j] / cnt_clu[j]\n",
    "            pu.append(pij)\n",
    "            if pij != 0:\n",
    "                en += -(pij * np.log2(pij))\n",
    "        max_pu = max(pu)\n",
    "        ds_max = []\n",
    "        for idx, p in enumerate(pu):\n",
    "            if p == max_pu:\n",
    "                ds_max.append(idxd[idx])\n",
    "        print(\n",
    "            'Cluster: {0} -- '\n",
    "            'Entropy: {1:.3f}, '\n",
    "            'Purity: {2:.3f}'.format(j, en, max_pu))\n",
    "        for d in ds_max:\n",
    "            print(\"max(P) in cluster disease {0}\".format(d))\n",
    "        cweight = cnt_clu[j] / len(gt)\n",
    "        entropy += cweight * en\n",
    "        purity += cweight * max_pu\n",
    "\n",
    "    print('Average Entropy: {0:.2f}'.format(entropy))\n",
    "    print('Average Purity: {0:.2f}'.format(purity))\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "#Input: ehr lists corresponding to a cluster \n",
    "#Output: dictionary of term counts\n",
    "def FreqDict(tokens):\n",
    "    freq_dict = {}\n",
    "    for seq in tokens:\n",
    "        for s in seq:\n",
    "            if s not in freq_dict:\n",
    "                freq_dict[s] = 1\n",
    "            else:\n",
    "                freq_dict[s] += 1\n",
    "    return freq_dict\n",
    "#Input: dictionary cluster:ehrs; list mrns\n",
    "#Output:\n",
    "def freq_term(data, pred_class):\n",
    "    list_terms = []\n",
    "    for subc in range(len(set(pred_class))):\n",
    "        tmp_data = {}\n",
    "        for j in range(len(pred_class)):\n",
    "            if pred_class[j] == subc:\n",
    "                tmp_data.setdefault(subc, list()).append([rd for rd in data[j] \n",
    "                                                           if rd!=0 and \n",
    "                                                           (str.split(vocab[str(rd)], \"::\")[0] \n",
    "                                                           in FRpar['ty_terms'])])\n",
    "        print(\"Cluster {0} numerosity: {1}\".format(subc, len(tmp_data[subc])))\n",
    "        term_count = FreqDict(tmp_data[subc])\n",
    "        clust_mostfreq = []\n",
    "        for l in range(FRpar['n_terms']):\n",
    "            try:\n",
    "                MFMT = max(term_count, key=(lambda key: term_count[key]))\n",
    "                num_MFMT = 0\n",
    "                n_subj = 0\n",
    "                for ehr in tmp_data[subc]:\n",
    "                    if MFMT in ehr:\n",
    "                        n_subj += 1\n",
    "                for _, seq in raw_ehr.items():\n",
    "                    for t in seq:\n",
    "                        if t == MFMT:\n",
    "                            num_MFMT += 1\n",
    "                print(\"% most frequent term:{1} \"\n",
    "                       \"= {2:.2f} ({3} out of {4} terms in the whole dataset\"\n",
    "                       \"-- N patients in cluster {5})\".format(subc,\n",
    "                                                             vocab[str(MFMT)], \n",
    "                                                             term_count[MFMT]/num_MFMT, \n",
    "                                                             term_count[MFMT],\n",
    "                                                             num_MFMT,\n",
    "                                                             n_subj))\n",
    "                term_count.pop(MFMT)\n",
    "                clust_mostfreq.append(MFMT)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        print(\"\\n\")\n",
    "        list_terms.append(clust_mostfreq)\n",
    "    return list_terms\n",
    "\n",
    "##Hierarchical clustering function. Max silhouette.\n",
    "def hclust_ehr(data, min_cl, max_cl, linkage, affinity):\n",
    "    best_silh = -1\n",
    "    list_silh = []\n",
    "    for nc in range(min_cl,max_cl,1):\n",
    "        hclust = AgglomerativeClustering(n_clusters=nc, \n",
    "                                         linkage=linkage, \n",
    "                                         affinity=affinity)\n",
    "        tmp_label = hclust.fit_predict(data).tolist()\n",
    "        tmp_silh = silhouette_score(data, tmp_label, metric=affinity)\n",
    "        print(nc, tmp_silh)\n",
    "        list_silh.append(float(tmp_silh))\n",
    "        if tmp_silh > best_silh:\n",
    "            best_silh = tmp_silh\n",
    "            n_clust = nc\n",
    "            label = tmp_label\n",
    "    try:\n",
    "        print(\"Number of clusters found:{0}, Silhouette score:{1:.3f}\\n\".format(n_clust, best_silh))\n",
    "    except UnboundLocalError:\n",
    "        hclust = AgglomerativeClustering(n_clusters=min_cl,\n",
    "                                         linkage=linkage,\n",
    "                                         affinity=affinity)\n",
    "        n_clust = min_cl\n",
    "        label = hclust.fit_predict(data).tolist()\n",
    "        best_silh = silhouette_score(data, label)\n",
    "        print(\"Number of clusters found:{0}, Silhouette score:{1:.3f}\\n\".format(n_clust, best_silh))\n",
    "    return n_clust, label, list_silh\n",
    "\n",
    "def chi_test(data, new_classes, term, mrns):\n",
    "    count_mat = np.zeros((2, len(set(new_classes))), dtype=int)\n",
    "    for c in set(new_classes):\n",
    "        for idx, m in enumerate(mrns):\n",
    "            if new_classes[idx] == c:\n",
    "                if term in data[idx]:\n",
    "                    count_mat[1][c] += 1\n",
    "                else:\n",
    "                    count_mat[0][c] += 1\n",
    "    print(\"Count matrix:\\n {0}\".format(count_mat))\n",
    "    chi2_stat, p_val, dof, ex = stats.chi2_contingency(count_mat)\n",
    "    string = \"Chi-squared test statistics: chi2_stat = {0} -- p_val = {1} -- dof = {2}\".format(\n",
    "                                                                  chi2_stat,\n",
    "                                                                  p_val,\n",
    "                                                                  dof)#row = classes, columns = vocab\n",
    "    print(string)\n",
    "    \n",
    "##Internal clustering validation\n",
    "def inner_clustering_analysis(disease_class, data, mrns, viz_data, preproc=False):\n",
    "    if preproc:\n",
    "        data = preprocessing.scale(data)\n",
    "    dis_viz_data = []\n",
    "    subclass_dis = []\n",
    "    for dis in sorted(set(disease_class)):\n",
    "        tmp_data = []\n",
    "        tmp_mrn = []\n",
    "        tmp_raw_ehr = []\n",
    "        for idx, d in enumerate(disease_class):\n",
    "            if d == dis:\n",
    "                dis_viz_data.append(viz_data[idx])\n",
    "                tmp_data.append(data[idx])\n",
    "                tmp_mrn.append(mrns[idx])\n",
    "                tmp_raw_ehr.append(raw_ehr[mrns[idx]])\n",
    "        print(\"Inspecting disease: {0}\\n\".format(dis))\n",
    "        n_clust, label, _ = hclust_ehr(tmp_data)\n",
    "        subclass_dis.extend([dis + ': subclust ' + str(l) for l in label])\n",
    "        list_terms = freq_term(tmp_raw_ehr, label)\n",
    "        for l in range(len(set(label))):\n",
    "            for lt in range(len(list_terms[l])):\n",
    "                print(\"Odds ratio chi2 test for cluster {0}\"\n",
    "                      \"term: {1}\".format(l, vocab[str(list_terms[l][lt])]))\n",
    "                try:\n",
    "                    chi_test(tmp_raw_ehr, label, list_terms[l][lt], tmp_mrn)\n",
    "                except ValueError:\n",
    "                    print(\"empty class(es)\")\n",
    "                    pass\n",
    "            print(\"\\n\\n\")\n",
    "    return(dis_viz_data, subclass_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_inspection(indir,\n",
    "                          outdir,\n",
    "                          expdir, \n",
    "                          disease_dt,\n",
    "                          sampling=None,\n",
    "                          exclude_oth=True):\n",
    "    \n",
    "    print(\"Loading datasets...\")\n",
    "    # get the list of diagnosed diseases associated with mrns\n",
    "    with open(path.join(indir, 'cohort-mrn_diseases.csv')) as f:\n",
    "        rd = csv.reader(f)\n",
    "        mrn_disease = {r[0]: r[1::] for r in rd}\n",
    "        \n",
    "    # read encoded vectors file and ordered medical record numbers\n",
    "    with open(path.join(expdir, 'mrns.csv')) as f:\n",
    "        rd = csv.reader(f)\n",
    "        mrns = [r[0] for r in rd]\n",
    "    \n",
    "    with open(path.join(expdir, 'encoded_vect.csv')) as f:\n",
    "        rd = csv.reader(f)\n",
    "        encoded = [list(map(float, r)) for r in rd]\n",
    "        \n",
    "        # sub-sample the collection\n",
    "    if n_samples is not None:\n",
    "        idx = [i for i in range(len(mrns))]\n",
    "        random.shuffle(idx)\n",
    "        idx = idx[:n_samples]\n",
    "        mrn_tmp = [mrns[i] for i in idx]\n",
    "        enc_tmp = [encoded[i] for i in idx]\n",
    "        mrns = mrn_tmp\n",
    "        encoded = enc_tmp\n",
    "    set_mrns = mrns\n",
    "\n",
    "    # (1) first diagnosis\n",
    "    gt_disease = {}\n",
    "    if exclude_oth:\n",
    "        for m in mrn_disease:\n",
    "            if mrn_disease[m][0]!='OTH' and m in set_mrns:\n",
    "                gt_disease[m] = mrn_disease[m][0]\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        for m in mrn_disease:\n",
    "            if m in set_mrns:\n",
    "                gt_disease[m] = mrn_disease[m][0]\n",
    "            else:\n",
    "                pass\n",
    "                    \n",
    "    # read the vocabulary\n",
    "    with open(path.join(indir, 'cohort-new_vocab.csv')) as f:    \n",
    "        rd = csv.reader(f)\n",
    "        next(rd)\n",
    "        vocab = {r[1]: r[0] for r in rd}\n",
    "    len_vocab = len(vocab)\n",
    "\n",
    "    # read raw data\n",
    "    with open(path.join(indir, 'cohort-new_ehr.csv')) as f:\n",
    "        rd = csv.reader(f)\n",
    "        raw_ehr = {}\n",
    "        for r in rd:\n",
    "            if r[0] in gt_disease.keys():\n",
    "                raw_ehr.setdefault(r[0], list()).extend(list(map(int, r[1::])))\n",
    "                \n",
    "    ##Read LSTM encoded vectors file and ordered medical record numbers\n",
    "    with open(expdir + '/LSTMencoded_vect.csv') as f:\n",
    "        rd = csv.reader(f)\n",
    "        lstm_encoded_vect = []\n",
    "        for r in rd:\n",
    "            lstm_encoded_vect.append(list(map(float, r)))\n",
    "        \n",
    "    with open(expdir + '/LSTMmrns.csv') as f:\n",
    "        rd = csv.reader(f)\n",
    "        lstm_mrns = [r[0] for r in rd]\n",
    "\n",
    "    tmp_mrns = []\n",
    "    tmp_lstm_mrns = []\n",
    "    tmp_encoded = []\n",
    "    tmp_lstm_encoded = []\n",
    "    for (idx, m), m_lstm in zip(enumerate(set_mrns), lstm_mrns):\n",
    "        if m in gt_disease.keys():\n",
    "            tmp_mrns.append(m)\n",
    "            tmp_encoded.append(encoded[idx])\n",
    "        elif m_lstm in gt_disease.keys():\n",
    "            tmp_lstm_mrns.append(m_lstm)\n",
    "            tmp_lstm_encoded.append(lstm_encoded_vect[idx])\n",
    "        else:\n",
    "            pass\n",
    "    set_mrns = tmp_mrns\n",
    "    lstm_mrns = tmp_lstm_mrns\n",
    "    encoded = tmp_encoded\n",
    "    lstm_encoded_vect = tmp_lstm_encoded\n",
    "\n",
    "    # raw data (scaled) counts\n",
    "    scaler = MinMaxScaler()\n",
    "    data = raw_ehr.values()\n",
    "    mrn_list = [m for m in raw_ehr.keys()]\n",
    "    raw_data = np.zeros((len(data), len_vocab))\n",
    "    for idx, token_list in enumerate(data):\n",
    "        for t in token_list:\n",
    "            raw_data[idx, t - 1] += 1\n",
    "\n",
    "    disease_count = {}\n",
    "    for d in gt_disease.values():\n",
    "        if d not in disease_count:\n",
    "            disease_count[d] = 1\n",
    "        else:\n",
    "            disease_count[d] += 1\n",
    "    print(\"Number of subjects:{0}\".format(len(gt_disease)))\n",
    "    print(\"Disease numerosities:\\n {0}\".format(disease_count))\n",
    "    \n",
    "    print(\"UMAP embeddings...\")\n",
    "    # initialize UMAP\n",
    "    reducer = umap.UMAP(n_neighbors=200, min_dist=0.5, metric = HCpar['affinity_clu'], n_components=2)\n",
    "\n",
    "    # plot colors\n",
    "    col_dict = matplotlib.colors.CSS4_COLORS\n",
    "    c_out = ['mintcream', 'cornsilk', 'lavenderblush', 'aliceblue', 'antiquewhite', 'aqua', 'aquamarine', 'azure', 'beige', 'powderblue', 'floralwhite', 'ghostwhite',\n",
    "     'lightcoral', 'lightcyan', 'lightgoldenrodyellow', 'lightgray', 'lightgreen', 'lightgrey', 'lightpink', 'lightsalmon', 'lightseagreen', 'lightskyblue',\n",
    "     'lightslategray', 'lightslategrey', 'lightsteelblue', 'lightyellow', 'linen', 'palegoldenrod', 'palegreen', 'paleturquoise', 'palevioletred', 'papayawhip',\n",
    "     'peachpuff', 'mistyrose', 'lemonchiffon', 'lightblue', 'seashell', 'white', 'blanchedalmond', 'oldlace', 'moccasin', 'snow', 'darkgray',\n",
    "     'ivory', 'whitesmoke']\n",
    "    colormap = [c for c in col_dict if c not in c_out]\n",
    "\n",
    "    \n",
    "    # UMAP on the CNN encoded vectors\n",
    "    encoded_umap = reducer.fit_transform(encoded).tolist()\n",
    "    print('Computed: CNN - AE encoded vectors umap')\n",
    "    # UMAP on the TF-IDF + SVD matrix\n",
    "    svd_mat = svd_count(raw_data, len_vocab, n_dimensions = 100)\n",
    "    count_umap = reducer.fit_transform(svd_mat).tolist()\n",
    "    print(\"Computed: COUNT matrix umap\")\n",
    "    # UMAP on the LSTM encoded vectors\n",
    "    lstm_encoded_umap = reducer.fit_transform(lstm_encoded_vect).tolist()\n",
    "    print(\"Computed: LSTM encoded vectors umap\")\n",
    "\n",
    "    # choose the disease classes: first_disease, oth_disease\n",
    "    disease_class_first = [gt_disease[m] for m in set_mrns]\n",
    "    raw_disease_class_first = [gt_disease[m] for m in mrn_list]\n",
    "    lstm_disease_class_first = [gt_disease[m] for m in lstm_mrns]\n",
    "    disease_dict = {d: i for i, d in enumerate(set(disease_class_first))}\n",
    "    \n",
    "    ##Parameters for CNN-AE and LSTM\n",
    "    HCpar = {'linkage_clu':'complete',\n",
    "             'affinity_clu':'cosine',\n",
    "             'min_cl':2,\n",
    "             'max_cl':11}\n",
    "    \n",
    "    print(\"Evaluating CNN-AE encodings...\")\n",
    "    ##CNN-AE encodings\n",
    "    # plot data\n",
    "    colors_en1 = [colormap[disease_dict[v]] for v in disease_class_first]\n",
    "    single_plot(encoded_umap, disease_class_first, colors_en1, \n",
    "                path.join(expdir, 'cnn-ae_encodings_plot.png'))\n",
    "    # plot cluster results\n",
    "    clusters = outer_clustering_analysis(encoded, disease_class_first, HCpar['linkage_clu'], HCpar['affinity_clu'], preproc=False)\n",
    "    colors_en2 = [colormap[v] for v in clusters]\n",
    "    single_plot(encoded_umap, clusters, colors_en2, \n",
    "                path.join(expdir, 'cnn-ae_outer-clust_plot.png'))\n",
    "    # inner clustering analysis\n",
    "    encoded_subplots, en_sub_clust = inner_clustering_analysis(disease_class_first, encoded, mrns, \n",
    "                                                               encoded_umap, preproc=False)\n",
    "    encoded_new_disease_dict = {}\n",
    "    for idx, nd in enumerate(set(en_sub_clust)):\n",
    "        encoded_new_disease_dict[nd] = idx\n",
    "    colors_en3 = [colormap[encoded_new_disease_dict[v]] for v in en_sub_clust]\n",
    "    single_plot(encoded_subplots, en_sub_clust, colors_en3, \n",
    "                path.join(expdir, 'cnn-ae_sub-clust_plot.png'))\n",
    " \n",
    "    print(\"Evaluating LSTM encodings...\")\n",
    "    ##LSTM encodings\n",
    "    # plot data\n",
    "    colors_lstm1 = [colormap[disease_dict[v]] for v in lstm_disease_class_first]\n",
    "    single_plot(lstm_encoded_umap, lstm_disease_class_first, colors_lstm1, \n",
    "                path.join(expdir, 'lstm_encodings_plot.png'))\n",
    "    # plot cluster results\n",
    "    clusters = outer_clustering_analysis(lstm_encoded_vect, lstm_disease_class_first, HCpar['linkage_clu'], \n",
    "                                         HCpar['affinity_clu'], preproc=False)\n",
    "    colors_lstm2 = [colormap[v] for v in clusters]\n",
    "    single_plot(lstm_encoded_umap, clusters, colors_lsmt2, \n",
    "                path.join(expdir, 'lstm_outer-clust_plot.png'))\n",
    "    lstm_subplots, lstm_sub_clust = inner_clustering_analysis(lstm_disease_class_first, lstm_encoded_vect, \n",
    "                                                             lstm_mrns, lstm_umap)\n",
    "    lstm_new_disease_dict = {}\n",
    "    for idx, nd in enumerate(set(lstm_sub_clust)):\n",
    "        lstm_new_disease_dict[nd] = idx\n",
    "    colors_lstm3 = [colormap[lstm_new_disease_dict[v]] for v in lstm_sub_clust]\n",
    "    single_plot(lstm_subplots, lstm_sub_clust, colors_lstm3, \n",
    "                path.join(expdir, 'lstm_sub-clust_plot.png'))\n",
    "    \n",
    "    ##Silhouette analysis\n",
    "    _,_,enc_silh = silhouette_analysis(encoded, HCpar['min_cl'], HCpar['max_cl'],\n",
    "                                       HCpar['linkage_clu'], HCpar['affinity_clu'])\n",
    "    _,_,lstm_silh = silhouette_analysis(lstm_encoded_vect, HCpar['min_cl'], HCpar['max_cl'],\n",
    "                                        HCpar['linkage_clu'], HCpar['affinity_clu'])\n",
    "    \n",
    "    ##Parameters for COUNT baseline\n",
    "    HCpar = {'linkage_clu':'ward',\n",
    "             'affinity_clu':'euclidean',\n",
    "             'min_cl':2,\n",
    "             'max_cl':11}\n",
    "    \n",
    "    print(\"Evaluating scaled COUNT matrix...\")\n",
    "    ##COUNT scaled matrix\n",
    "    # plot data\n",
    "    colors_count1 = [colormap[disease_dict[v]] for v in raw_disease_class_first]\n",
    "    single_plot(count_umap, raw_disease_class_first, colors_count1, \n",
    "                path.join(expdir, 'count_encodings_plot.png'))\n",
    "    # plot cluster results\n",
    "    clusters = outer_clustering_analysis(svd_mat, raw_disease_class_first, HCpar['linkage_clu'], \n",
    "                                         HCpar['affinity_clu'], preproc=False)\n",
    "    colors_count2 = [colormap[v] for v in clusters]\n",
    "    single_plot(count_umap, clusters, colors_count2, \n",
    "                path.join(expdir, 'count_outer-clust_plot.png'))\n",
    "    count_subplots, count_sub_clust = inner_clustering_analysis(raw_disease_class_first, svd_mat, \n",
    "                                                                mrn_list, count_umap)\n",
    "    count_new_disease_dict = {}\n",
    "    for idx, nd in enumerate(set(count_sub_clust)):\n",
    "        count_new_disease_dict[nd] = idx\n",
    "    colors_count3 = [colormap[count_new_disease_dict[v]] for v in count_sub_clust]\n",
    "    single_plot(count_subplots, count_sub_clust, colors_count3, \n",
    "                path.join(expdir, 'count_sub-clust_plot.png'))\n",
    "    \n",
    "    ##Silhouette analysis\n",
    "    _,_,count_silh = silhouette_analysis(svd_mat,HCpar['min_cl'], HCpar['max_cl'],\n",
    "                                         HCpar['likage_clu'], HCpar['affinity_clu'])\n",
    "    \n",
    "    print(\"Plotting silhouettes for model and baselines:\")\n",
    "    minmax_clust = [r for r in range(HCpar['min_cl'], HCpar['max_cl'])]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(enc_silh, minmax_clust, '-o')\n",
    "    ax.plot(lstm_silh, minmax_clust, '-o')\n",
    "    ax.plot(count_silh, minmax_clust, '-o')\n",
    "    ax.legen(['cnn-ae', 'lstm', 'count matrix'])\n",
    "    fig.savefig(path.join(expdir, 'silhouette_plot.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='EHR Patient Stratification: perform hierarchical clustering '\n",
    "        'validate results and inspect subclusters.')\n",
    "    parser.add_argument(dest='indir', help='EHR dataset directory')\n",
    "    parser.add_argument(dest='expdir', help='Experiment directory')\n",
    "    parser.add_argument(dest='disease_dt', help='Disease dataset name')\n",
    "    parser.add_argument('-s', default=None, type=int,\n",
    "                        help='Enable sub-sampling with data size '\n",
    "                        '(defaut: None)'\n",
    "    return parser.parse_args(sys.argv[1:])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = _process_args()\n",
    "    print ('')\n",
    "\n",
    "    start = time()\n",
    "    clustering_inspection(indir=args.indir,\n",
    "                          expdir=args.expdir,\n",
    "                          disease_dt=args.disease_dt,\n",
    "                          sampling=args.s)\n",
    "\n",
    "    print ('\\nProcessing time: %s seconds\\n' % round(time() - start, 2))\n",
    "\n",
    "    print ('Task completed\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
